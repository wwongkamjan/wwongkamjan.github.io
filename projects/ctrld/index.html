<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/intro_example.pdf" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/intro_example.pdf">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/F1903M.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://wwongkamjan.github.io/" target="_blank">Wichayaporn Wongkamjan</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.isi.edu/directory/yanze-wang/" target="_blank">Yanze Wang</a><sup>4</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=PeavuGUAAAAJ" target="_blank">Feng Gu</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://denis.ai/" target="_blank">Denis Peskoff</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://jkk.name/" target="_blank">Jonathan K. Kummerfeld</a><sup>3</sup>,
                  </span>
                  <span class="author-block">
                <a href="https://www.isi.edu/~jonmay/" target="_blank">Jonathan May</a><sup>4</sup>,</span>
                            <span class="author-block">
                <a href="https://users.umiacs.umd.edu/~jbg/" target="_blank">Jordan Lee Boyd-Graber</a><sup>1</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Maryland,</span> 
                    <span class="author-block"><sup>2</sup>Northwestern University,</span> 
                    <span class="author-block"><sup>3</sup>University of Sydney,</span>
                    <span class="author-block"><sup>4</sup>ISI, University of Southern California,</span> 
                   </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><br>ACL Findings, 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://aclanthology.org/2025.findings-acl.1287.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ALLAN-DIP/diplomacy_cicero/tree/deception_friction_value/fairdiplomacy_external/friction" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-vcentered is-centered">
      
      <div class="column is-half has-text-centered">
        <img src="static/images/motivate.png" alt="Motivating example" style="max-width: 100%; height: auto;" />
      </div>

      <div class="column is-half">
        <div class="content has-text-justified">
          <p>
            An increasingly common socio-technical problem is people being taken in by offers that
            sound ‚Äútoo good to be true‚Äù, where persuasion
            and trust shape decision-making. This paper
            investigates how AI can help detect these deceptive scenarios. We analyze how humans
            strategically deceive each other in Diplomacy,
            a board game that requires both natural language communication and strategic reasoning.
            This requires extracting logical forms representing proposals‚Äîagreements that players suggest
            during communication‚Äîand computing their
            relative rewards using agents‚Äô value functions.
            Combined with text-based features, this can
            improve our deception detection. Our method
            detects human deception with a high precision
            when compared to a Large Language Model
            approach that flags many true messages as deceptive. Future human-AI interaction tools can
            build on our methods for deception detection
            by triggering friction to give users a chance of
            interrogating suspicious proposals.
          </p>
        </div>
      </div>

    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-full" style="max-width: 100%;">
          Diplomacy offers a rich environment to study deception because strategic betrayal has tangible consequences.
        </h2>
        <img src="static/images/F1903M.png" alt="Diplomacy game" style="max-width: 100%; height: auto; margin: 1rem auto;" />
        <div class="content has-text-justified" style="margin-top: 1rem;">
          <p>
            Diplomacy is a unique environment that combines natural language communication with strategic gameplay, making it a compelling testbed for studying deception. Deception in this game has real consequences‚Äîplayers who fall for misleading proposals often lose key territories, while deceivers gain strategic advantages. The game‚Äôs structured yet open-ended setting provides a controlled environment to explore trust, persuasion, and betrayal, all of which are relevant to real-world scenarios involving misinformation. As AI systems increasingly participate in human-facing interactions, detecting and mitigating deception‚Äîespecially when it arises strategically‚Äîbecomes a necessary and impactful problem.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-full" style="max-width: 100%;">
          We quantify deception through three metrics---bait, switch, and edge---that capture the risks and benefits of betrayal.
        </h2>
        <img src="static/images/overview (1).png" alt="Diplomacy game" style="max-width: 100%; height: auto; margin: 1rem auto;" />
        <div class="content has-text-justified" style="margin-top: 1rem;">
          <p>
            The paper introduces three novel metrics to characterize deception in strategic proposals: Bait, Switch, and Edge. "Bait" measures how much better off a victim appears if they trust the deceiver; "Switch" captures how much worse the victim becomes if the deceiver betrays them instead of cooperating; and "Edge" quantifies the benefit the deceiver gains from betrayal. These signals are grounded in value differences from counterfactual outcomes. For example, Figure 2 in the paper demonstrates how Austria's seemingly supportive message to Italy is actually a setup for betrayal, with all three metrics quantifying this deception.
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-full" style="max-width: 100%;">
          CTRL-D combines language understanding and counterfactual reasoning to detect deceptive negotiation proposals.
        </h2>
        <div class="content has-text-justified" style="margin-top: 1rem;">
          <p>
            CTRL-D (CounTerfactual RL against Deception) is a novel pipeline that begins by parsing negotiation messages into logical proposals using Abstract Meaning Representation (AMR). These proposals are then evaluated using CICERO‚Äôs reinforcement learning-based value function to compute bait, switch, and edge. These numerical indicators are combined with BERT-based textual embeddings and fed into a neural network classifier. This hybrid model enables robust detection of deceptive proposals, effectively surfacing hidden strategies that would be missed by language-only models.
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-full" style="max-width: 100%;">
          Our experiments use real annotated data and compare against Llama 3 zero-shot prompting baselines.
        </h2>
        <div class="content has-text-justified" style="margin-top: 1rem;">
          <p>
            To evaluate CTRL-D, the authors use two datasets: one from human-only Diplomacy games annotated with deception labels (Peskov et al., 2020), and a large-scale human-AI interaction dataset from webdiplomacy.net. For comparison, they benchmark against both traditional baselines (like LSTM with power dynamics) and LLM-based deception detection using LLaMA 3.1. Importantly, CTRL-D focuses on high-precision alerts to create strategic friction, helping players reevaluate risky decisions. While deceptive moves are rare (<2%), the framework is designed to flag suspicious messages without overwhelming users with false positives.
        </div>
      </div>
    </div>
  </div>
</section>

    <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-full" style="max-width: 100%;">
          CTRL-D achieves high-precision deception detection, outperforming LLMs by integrating strategic reasoning.
        </h2>
        <img src="static/images/table.png" alt="Diplomacy game" style="max-width: 100%; height: auto; margin: 1rem auto;" />
        <div class="content has-text-justified" style="margin-top: 1rem;">
          <p>
            CTRL-D achieves remarkably high precision (0.95) in detecting human-labeled lies, significantly outperforming both LLM baselines and prior methods. Though its recall is moderate (0.238), this tradeoff is acceptable given the rarity and difficulty of identifying deceptive acts. In large-scale experiments, CTRL-D remains conservative but precise, triggering alerts only when deception is highly likely. The results validate that deception in language can be detected more reliably when contextualized through game dynamics and reinforcement learning signals, offering a new direction for safer human-AI collaboration in negotiation settings.
      </div>
    </div>
  </div>
</section>

  <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-full" style="max-width: 100%;">
          Interests in more of our works? 
        </h2>
        <div class="content has-text-justified" style="margin-top: 1rem;">
          <p>
            <ul>
                <li>How we ground the language and detect persuasion and deception when humans and AI play Diplomacy! <a href="https://aclanthology.org/2024.acl-long.672/" target="_blank">üìù (paper)</a> </li>
                <li>How the best AI Diplomacy player can help noobs play Diplomacy! <a href="https://aclanthology.org/2025.naacl-short.6/" target="_blank">üìù (paper)</a> </li>
            </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<!-- <section class="hero is-small is-light"> -->
<!--   <div class="hero-body"> -->
<!--     <div class="container"> -->
      <!-- Paper video. -->
<!--       <h2 class="title is-3">Video Presentation</h2> -->
<!--       <div class="columns is-centered has-text-centered"> -->
<!--         <div class="column is-four-fifths"> -->
          
<!--           <div class="publication-video"> -->
            <!-- Youtube embed code here -->
<!--             <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
<!--           </div> -->
<!--         </div> -->
<!--       </div> -->
<!--     </div> -->
<!--   </div> -->
<!-- </section> -->
<!-- End youtube video
 -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{wongkamjan-etal-2025-trust,
    title = "Should {I} Trust You? Detecting Deception in Negotiations using Counterfactual {RL}",
    author = "Wongkamjan, Wichayaporn  and
      Wang, Yanze  and
      Gu, Feng  and
      Peskoff, Denis  and
      Kummerfeld, Jonathan K.  and
      May, Jonathan  and
      Boyd-Graber, Jordan Lee",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1287/",
    pages = "25099--25113",
    ISBN = "979-8-89176-256-5",
    abstract = "An increasingly common socio-technical problem is people being taken in by offers that sound ``too good to be true'', where persuasion and trust shape decision-making. This paper investigates how AI can help detect these deceptive scenarios. We analyze how humans strategically deceive each other in Diplomacy, a board game that requires both natural language communication and strategic reasoning. This requires extracting logical forms representing proposals{---}agreements that players suggest during communication{---}and computing their relative rewards using agents' value functions. Combined with text-based features, this can improve our deception detection. Our method detects human deception with a high precision when compared to a Large Language Model approach that flags many true messages as deceptive. Future human-AI interaction tools can build on our methods for deception detection by triggering friction to give users a chance of interrogating suspicious proposals."
}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
